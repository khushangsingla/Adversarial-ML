{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4eda7ff-b1c6-4f3c-91dd-8d03226bd76e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130fe35b-d4a3-48b9-ae6c-7116ebe11268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:29:25.127375Z",
     "iopub.status.busy": "2023-11-28T17:29:25.126658Z",
     "iopub.status.idle": "2023-11-28T17:29:26.713725Z",
     "shell.execute_reply": "2023-11-28T17:29:26.712222Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from typing import Any,Tuple,Optional,Callable\n",
    "import PIL\n",
    "import csv\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "from torchvision.transforms import ToTensor,Resize,Compose,ColorJitter,RandomRotation,AugMix,RandomCrop,GaussianBlur,RandomEqualize,RandomHorizontalFlip,RandomVerticalFlip\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5dd3f-6a08-4cfc-afcb-6b5ab2e7faf5",
   "metadata": {},
   "source": [
    "### Select device to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4d21f1-1f24-4d1d-ab3d-f7fc1e6e6c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:29:26.720155Z",
     "iopub.status.busy": "2023-11-28T17:29:26.719847Z",
     "iopub.status.idle": "2023-11-28T17:29:26.772878Z",
     "shell.execute_reply": "2023-11-28T17:29:26.771924Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957a094-feea-416b-ab12-1cd5d2e5836c",
   "metadata": {},
   "source": [
    "## GTSRB Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8d7f9d-f8dd-4088-98b5-74f67ae13da0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:29:26.778780Z",
     "iopub.status.busy": "2023-11-28T17:29:26.778313Z",
     "iopub.status.idle": "2023-11-28T17:29:26.787049Z",
     "shell.execute_reply": "2023-11-28T17:29:26.786078Z"
    }
   },
   "outputs": [],
   "source": [
    "class GTSRB(Dataset):\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 split: str,\n",
    "                 transform: Optional[Callable] = None):\n",
    "       \n",
    "        \n",
    "        \n",
    "        self.base_folder = pathlib.Path(root)\n",
    "        self.csv_file = self.base_folder / ('Train.csv' if split =='train' else 'Test.csv')\n",
    "        \n",
    "        \n",
    "        with open(str(self.csv_file)) as csvfile:\n",
    "           samples = [(str(self.base_folder / row['Path']),int(row['ClassId'])) \n",
    "            for row in csv.DictReader(csvfile,delimiter=',',skipinitialspace=True)\n",
    "                ]\n",
    "\n",
    "\n",
    "        self.samples = samples\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return 20\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple:\n",
    "        path,classId =  self.samples[index]\n",
    "        sample = PIL.Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample,classId\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1aec28-b612-415f-88cb-a49f3e2958ae",
   "metadata": {},
   "source": [
    "## Load pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c64370-6f18-4203-94d2-576f54c22b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:29:26.793338Z",
     "iopub.status.busy": "2023-11-28T17:29:26.792548Z",
     "iopub.status.idle": "2023-11-28T17:29:26.819471Z",
     "shell.execute_reply": "2023-11-28T17:29:26.818604Z"
    }
   },
   "outputs": [],
   "source": [
    "class GTSRB_MODEL(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(GTSRB_MODEL,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "      \n",
    "        self.metrics = {}\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "       \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3)\n",
    "        self.conv6 = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "       \n",
    "       \n",
    "\n",
    "        self.l1 = nn.Linear(1024*4*4,512)\n",
    "        self.l2 = nn.Linear(512,128)\n",
    "        self.batchnorm4 = nn.LayerNorm(128)\n",
    "        self.l3 = nn.Linear(128,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        conv = self.conv1(input)\n",
    "        conv = self.conv2(conv)\n",
    "        batchnorm = self.relu(self.batchnorm1(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv3(maxpool)\n",
    "        conv = self.conv4(conv)\n",
    "        batchnorm = self.relu(self.batchnorm2(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv5(maxpool)\n",
    "        conv = self.conv6(conv)\n",
    "        batchnorm = self.relu(self.batchnorm3(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        flatten = self.flatten(maxpool)\n",
    "        \n",
    "        dense_l1 = self.l1(flatten)\n",
    "        dropout = self.dropout3(dense_l1)\n",
    "        dense_l2 = self.l2(dropout)\n",
    "        batchnorm = self.batchnorm4(dense_l2)\n",
    "        dropout = self.dropout2(batchnorm)\n",
    "        output = self.l3(dropout)\n",
    "        \n",
    "       \n",
    "        return output\n",
    "    \n",
    "    def training_metrics(self,positives,data_size,loss):\n",
    "        acc = positives/data_size\n",
    "        return loss,acc\n",
    "    \n",
    "    def validation_metrics(self,validation_data,loss_function):\n",
    "       data_size = len(validation_data)\n",
    "       correct_predictions = 0\n",
    "       total_samples = 0\n",
    "       val_loss = 0\n",
    "\n",
    "       model = self.eval()\n",
    "       with torch.no_grad() : \n",
    "        for step,(input,label) in enumerate(validation_data):\n",
    "            input,label = input.to(device),label.to(device)\n",
    "            prediction = model.forward(input)\n",
    "            loss = loss_function(prediction,label)\n",
    "            val_loss = loss.item()\n",
    "            _,predicted = torch.max(prediction,1)\n",
    "            correct_predictions += (predicted == label).sum().item()\n",
    "            total_samples += label.size(0)\n",
    "\n",
    "       val_acc = correct_predictions/total_samples\n",
    "\n",
    "       return val_loss,val_acc\n",
    "\n",
    "    def history(self):\n",
    "        return self.metrics\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def compile(self,train_data,validation_data,epochs,loss_function,optimizer,learning_rate_scheduler):\n",
    "        val_acc_list = []\n",
    "        val_loss_list = []\n",
    "\n",
    "        train_acc_list = []\n",
    "        train_loss_list = []\n",
    "\n",
    "        learning_rate_list = []\n",
    "\n",
    "        print('training started ...')\n",
    "        STEPS = len(train_data)\n",
    "        for epoch in range(epochs):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            learning_rate_list.append(lr)\n",
    "            correct_predictions = 0\n",
    "            total_examples = 0\n",
    "            loss = 0\n",
    "            with tqdm.trange(STEPS) as progress:\n",
    "\n",
    "                for step,(input,label) in enumerate(train_loader):\n",
    "\n",
    "                    input,label = input.to(device),label.to(device)\n",
    "                    prediction = self.forward(input)\n",
    "\n",
    "                    _, predicted = torch.max(prediction, 1)\n",
    "                    correct_predictions += (predicted == label).sum().item()\n",
    "                    total_examples += label.size(0)\n",
    "                    l = loss_function(prediction,label)\n",
    "                    loss = l.item()\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    progress.colour = 'green'\n",
    "                    progress.desc = f'Epoch [{epoch}/{EPOCHS}], Step [{step}/{STEPS}], Learning Rate [{lr}], Loss [{\"{:.4f}\".format(l)}], Accuracy [{\"{:.4f}\".format(correct_predictions/total_examples)}]'\n",
    "                    progress.update(1)\n",
    "\n",
    "            training_loss,training_acc = self.training_metrics(correct_predictions,total_examples,loss)\n",
    "            train_acc_list.append(training_acc)\n",
    "            train_loss_list.append(training_loss)\n",
    "\n",
    "            val_loss, val_acc = self.validation_metrics(validation_data,loss_function)\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            print(f'val_accuracy [{val_acc}], val_loss [{val_loss}]')\n",
    "\n",
    "            \n",
    "            learning_rate_scheduler.step()\n",
    "        \n",
    "        metrics_dict = {\n",
    "                'train_acc':train_acc_list,\n",
    "                'train_loss':train_loss_list,\n",
    "                'val_acc':val_acc_list,\n",
    "                'val_loss':val_loss_list,\n",
    "                'learning_rate':optimizer.param_groups[0][\"lr\"]\n",
    "            }\n",
    "        self.metrics = metrics_dict\n",
    "        print('training complete !')    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32cb3c34-4198-441d-ad45-fcc0eafb7600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:29:26.824496Z",
     "iopub.status.busy": "2023-11-28T17:29:26.824321Z",
     "iopub.status.idle": "2023-11-28T17:29:30.073371Z",
     "shell.execute_reply": "2023-11-28T17:29:30.071470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTSRB_MODEL(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (dropout3): Dropout(p=0.3, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batchnorm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Linear(in_features=16384, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (batchnorm4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (l3): Linear(in_features=128, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('gstrb-99-saved_model.pkl','rb')\n",
    "model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e63092-b589-4660-824e-f4b8945f6045",
   "metadata": {},
   "source": [
    "## FGSM Attack Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6cbafe-f99d-4000-ab01-bd0345654629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:29:30.080641Z",
     "iopub.status.busy": "2023-11-28T17:29:30.079999Z",
     "iopub.status.idle": "2023-11-28T17:29:30.089009Z",
     "shell.execute_reply": "2023-11-28T17:29:30.087753Z"
    }
   },
   "outputs": [],
   "source": [
    "def fgsm_attack_l2(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    normalized_data_grad = data_grad / data_grad.norm()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*normalized_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98be77-f3d7-4204-810b-626940acace9",
   "metadata": {},
   "source": [
    "## L2 FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e00fb2f-f298-4eca-9d24-382c3456c8e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:29:30.095621Z",
     "iopub.status.busy": "2023-11-28T17:29:30.094928Z",
     "iopub.status.idle": "2023-11-28T17:39:18.752982Z",
     "shell.execute_reply": "2023-11-28T17:39:18.751444Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|\u001b[31m███████████████████████████████████\u001b[0m| 39209/39209 [09:42<00:00, 67.26it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generate_adversarial_dataset_fgsm_l2(model, device, test_dataloader, epsilon):\n",
    "    adv_examples = []\n",
    "\n",
    "    with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "        # Loop over all examples in test set\n",
    "        for data, target in test_dataloader:\n",
    "            # Send the data and label to the device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Set requires_grad attribute of tensor. Important for Attack\n",
    "            data.requires_grad = True\n",
    "            # Forward pass the data through the model\n",
    "            output = model(data)\n",
    "            init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "            # if init_pred.item() != target.item():\n",
    "            #     adv_examples.append((data.squeeze().detach().cpu(), target))\n",
    "            #     progress.update(1)\n",
    "            #     continue\n",
    "            # Calculate the loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            # Calculate gradients of model in backward pass\n",
    "            loss.backward()\n",
    "            # Collect ``datagrad``\n",
    "            data_grad = data.grad.data\n",
    "            # Call FGSM Attack\n",
    "            perturbed_data = fgsm_attack_l2(data, epsilon, data_grad)\n",
    "            # Re-classify the perturbed image\n",
    "            output = model(perturbed_data)\n",
    "            adv_ex = perturbed_data.squeeze().detach().cpu()\n",
    "            adv_examples.append((adv_ex, target.detach().cpu().item()))\n",
    "            progress.desc = f'Progress: '\n",
    "            progress.update(1)\n",
    "\n",
    "    save_loc = f\"./pkl2/L2_epsilon2_train_advdata.pkl\"\n",
    "    with open(save_loc,'wb') as output_file:\n",
    "        pickle.dump(adv_examples,output_file)\n",
    "    return adv_examples\n",
    "\n",
    "transforms = Compose([\n",
    "    Resize([50,50]),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "traindataset = GTSRB(root='dataset',split=\"train\", transform=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(traindataset)\n",
    "\n",
    "adv_data = generate_adversarial_dataset_fgsm_l2(model, device, train_dataloader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c510406c-1d47-4722-a2de-71f3c856a875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:39:18.761396Z",
     "iopub.status.busy": "2023-11-28T17:39:18.760652Z",
     "iopub.status.idle": "2023-11-28T17:39:18.873278Z",
     "shell.execute_reply": "2023-11-28T17:39:18.872355Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0008\n",
    "INPUT_DIM = 3*50*50\n",
    "OUTPUT_DIM = 43\n",
    "model = GTSRB_MODEL(INPUT_DIM,OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = Adam(params=model.parameters(),lr=LEARNING_RATE)\n",
    "lr_s = lr_scheduler.LinearLR(optimizer,start_factor=1.0,end_factor=0.5,total_iters=10)\n",
    "loss = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea747715-e717-4e02-837a-a41a27e6ab4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:39:18.879151Z",
     "iopub.status.busy": "2023-11-28T17:39:18.878779Z",
     "iopub.status.idle": "2023-11-28T17:39:18.883406Z",
     "shell.execute_reply": "2023-11-28T17:39:18.882628Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(dataset,train_size):\n",
    "\n",
    "    train_size = int(train_size * len(dataset))\n",
    "    test_size = int(len(dataset) - train_size)\n",
    "    return random_split(dataset,[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed8cb2b-675f-4e0d-9a72-1056e08c49a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:39:18.889107Z",
     "iopub.status.busy": "2023-11-28T17:39:18.888777Z",
     "iopub.status.idle": "2023-11-28T17:39:19.155487Z",
     "shell.execute_reply": "2023-11-28T17:39:19.154796Z"
    }
   },
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1),\n",
    "    RandomEqualize(0.4),\n",
    "    AugMix(),\n",
    "    RandomHorizontalFlip(0.3),\n",
    "    RandomVerticalFlip(0.3),\n",
    "    GaussianBlur((3,3)),\n",
    "    RandomRotation(30),\n",
    "\n",
    "    Resize([50,50]),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "validation_transforms =  Compose([\n",
    "    Resize([50,50]),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "dataset = GTSRB(root='dataset',split=\"train\")\n",
    "train_set,validation_set = train_test_split(dataset,train_size=0.8)\n",
    "train_set.dataset.transform = train_transforms\n",
    "validation_set.dataset.transform = validation_transforms\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_set,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec448c8-8ca3-4a35-87db-89e20ec9072e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:39:19.165872Z",
     "iopub.status.busy": "2023-11-28T17:39:19.165494Z",
     "iopub.status.idle": "2023-11-28T17:43:17.080568Z",
     "shell.execute_reply": "2023-11-28T17:43:17.079404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Step [490/491], Learning Rate [0.0008], Loss [0.1597], Accuracy [0.6535]: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9817648559041061], val_loss [0.16771015524864197]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [490/491], Learning Rate [0.00076], Loss [0.0213], Accuracy [0.9882]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9838051517470033], val_loss [0.05901744216680527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [490/491], Learning Rate [0.00072], Loss [0.0034], Accuracy [0.9975]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9803621525121142], val_loss [0.014535217545926571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [490/491], Learning Rate [0.00068], Loss [0.0032], Accuracy [0.9970]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9964294822749299], val_loss [0.0073509011417627335]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [490/491], Learning Rate [0.00064], Loss [0.0005], Accuracy [0.9999]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.998342259627646], val_loss [0.0011955213267356157]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [490/491], Learning Rate [0.0006000000000000001], Loss [0.0014], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9987248150981892], val_loss [0.0005142829613760114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [490/491], Learning Rate [0.0005600000000000001], Loss [0.0001], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9988523335883703], val_loss [0.00031794901588000357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [490/491], Learning Rate [0.0005200000000000001], Loss [0.0001], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9991073705687324], val_loss [0.00024619593750685453]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [490/491], Learning Rate [0.00048000000000000007], Loss [0.0001], Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9991073705687324], val_loss [0.00019575665646698326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [490/491], Learning Rate [0.00044000000000000007], Loss [0.0001], Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.9992348890589136], val_loss [0.0001479537895647809]\n",
      "training complete !\n"
     ]
    }
   ],
   "source": [
    "model.compile(train_data=train_loader,validation_data=validation_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer,learning_rate_scheduler=lr_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e454bc-a0df-4f29-a1a1-29224d6f027b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:43:17.088036Z",
     "iopub.status.busy": "2023-11-28T17:43:17.087599Z",
     "iopub.status.idle": "2023-11-28T17:43:20.819435Z",
     "shell.execute_reply": "2023-11-28T17:43:20.818497Z"
    }
   },
   "outputs": [],
   "source": [
    "adv_data_file = open('./pkl2/L2_epsilon2_train_advdata.pkl','rb')\n",
    "adv_data = pickle.load(adv_data_file)\n",
    "adv_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354d16d4-a825-4817-9f08-a5f13d55a2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:43:20.826893Z",
     "iopub.status.busy": "2023-11-28T17:43:20.826632Z",
     "iopub.status.idle": "2023-11-28T17:43:20.835634Z",
     "shell.execute_reply": "2023-11-28T17:43:20.834781Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdversarialDataset:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx][0],self.data[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6b1e5e7-c593-4f28-a10e-c1e1cdda07e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:43:20.841361Z",
     "iopub.status.busy": "2023-11-28T17:43:20.841024Z",
     "iopub.status.idle": "2023-11-28T17:43:20.849682Z",
     "shell.execute_reply": "2023-11-28T17:43:20.848293Z"
    }
   },
   "outputs": [],
   "source": [
    "train_adv_dataset = AdversarialDataset(adv_data[:31000])\n",
    "val_adv_dataset = AdversarialDataset(adv_data[31000:])\n",
    "train_loader = DataLoader(train_adv_dataset, batch_size = 64,shuffle=True)\n",
    "val_loader = DataLoader(val_adv_dataset, batch_size = 64,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e9dbe26-2008-45d0-93dc-e4a64bf55ca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:43:20.856161Z",
     "iopub.status.busy": "2023-11-28T17:43:20.855486Z",
     "iopub.status.idle": "2023-11-28T17:45:11.005837Z",
     "shell.execute_reply": "2023-11-28T17:45:11.004790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0429], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.22316969180168109], val_loss [2.06939435005188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0294], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.18443172128152027], val_loss [3.6758553981781006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0095], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.17005725423315873], val_loss [3.152139186859131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0106], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.1381410646851017], val_loss [3.3734188079833984]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0002], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.13643561944207577], val_loss [4.236685276031494]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0001], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.129613838469972], val_loss [3.366760730743408]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0001], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.12985747350468999], val_loss [4.53563928604126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0001], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.13034474357412595], val_loss [4.035635948181152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0001], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.13387745157753686], val_loss [2.862488269805908]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [484/485], Learning Rate [0.0004000000000000001], Loss [0.0000], Accu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.1354610793032038], val_loss [4.397650241851807]\n",
      "training complete !\n"
     ]
    }
   ],
   "source": [
    "model.compile(train_data=train_loader,validation_data=val_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer,learning_rate_scheduler=lr_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5400c196-bb56-42e8-9ab2-4516da8e3358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:45:11.012323Z",
     "iopub.status.busy": "2023-11-28T17:45:11.011911Z",
     "iopub.status.idle": "2023-11-28T17:45:11.272181Z",
     "shell.execute_reply": "2023-11-28T17:45:11.270605Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('robust_model.pkl','wb') as outfile:\n",
    "    pickle.dump(model.cpu(),outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7265a93-d71a-4d39-ada7-0834efa26e12",
   "metadata": {},
   "source": [
    "## Test accuracy on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a91507ab-3472-4840-adaf-fd8eb26a7daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:45:11.279876Z",
     "iopub.status.busy": "2023-11-28T17:45:11.279116Z",
     "iopub.status.idle": "2023-11-28T17:45:58.692223Z",
     "shell.execute_reply": "2023-11-28T17:45:58.691228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing size : 12630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.8038796516231196 : 100%|\u001b[31m███████\u001b[0m| 12630/12630 [00:47<00:00, 267.11it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose([\n",
    "    Resize([50,50]),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "\n",
    "testdata = GTSRB(root='./dataset',split='test',transform=transforms)\n",
    "print('testing size :',len(testdata))\n",
    "test_dataloader = DataLoader(testdata)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model = model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input,label = input.to(device),label.to(device)\n",
    "        y_true.append(label.item())\n",
    "        prediction = model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred.append(prediction.item())\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d0e18-00b6-4c1f-9f15-70735ab0830e",
   "metadata": {},
   "source": [
    "### Dataset class for loading adversarial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6437e66-2700-4668-bd6d-f896b68d950a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:45:58.699903Z",
     "iopub.status.busy": "2023-11-28T17:45:58.699510Z",
     "iopub.status.idle": "2023-11-28T17:45:58.704961Z",
     "shell.execute_reply": "2023-11-28T17:45:58.704162Z"
    }
   },
   "outputs": [],
   "source": [
    "class GTSRB_adv(Dataset):\n",
    "    def __init__(self,file):\n",
    "        f = open(file,'rb')\n",
    "        self.data = pickle.load(f)\n",
    "        f.close()\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx][0],self.data[idx][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7b82c-b33a-4efc-b799-36aab0a657dc",
   "metadata": {},
   "source": [
    "## Test accuracy on L2 adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb0d00b-5564-495b-ba18-28a0b7e131e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:45:58.710734Z",
     "iopub.status.busy": "2023-11-28T17:45:58.710290Z",
     "iopub.status.idle": "2023-11-28T17:46:45.404943Z",
     "shell.execute_reply": "2023-11-28T17:46:45.403857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing size : 12630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.7992874109263658 : 100%|\u001b[31m███████\u001b[0m| 12630/12630 [00:43<00:00, 290.63it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "testdata = GTSRB_adv('./pkl/2_generate_adversarial_dataset_fgsm_l2_{}.pkl')\n",
    "print('testing size :',len(testdata))\n",
    "test_dataloader = DataLoader(testdata)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model = model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input,label = input.to(device),label.to(device)\n",
    "        y_true.append(label.item())\n",
    "        prediction = model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred.append(prediction.item())\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bd6cd-c321-4697-ab4f-1d81b4094425",
   "metadata": {},
   "source": [
    "## Test accuracy on small multistep adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f539898-a1cc-41fe-9d19-bb38a1230a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:46:45.412825Z",
     "iopub.status.busy": "2023-11-28T17:46:45.412370Z",
     "iopub.status.idle": "2023-11-28T17:47:31.328343Z",
     "shell.execute_reply": "2023-11-28T17:47:31.327315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing size : 12630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.7904196357878068 : 100%|\u001b[31m███████\u001b[0m| 12630/12630 [00:42<00:00, 294.13it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "testdata = GTSRB_adv(\"./pkl/2_generate_adversarial_dataset_fgsm_multistep_{'num_epochs': 20}.pkl\")\n",
    "print('testing size :',len(testdata))\n",
    "test_dataloader = DataLoader(testdata)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model = model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input,label = input.to(device),label.to(device)\n",
    "        y_true.append(label.item())\n",
    "        prediction = model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred.append(prediction.item())\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
