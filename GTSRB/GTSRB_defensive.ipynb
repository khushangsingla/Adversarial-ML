{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4eda7ff-b1c6-4f3c-91dd-8d03226bd76e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130fe35b-d4a3-48b9-ae6c-7116ebe11268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from typing import Any,Tuple,Optional,Callable\n",
    "import PIL\n",
    "import csv\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "from torchvision.transforms import ToTensor,Resize,Compose,ColorJitter,RandomRotation,AugMix,RandomCrop,GaussianBlur,RandomEqualize,RandomHorizontalFlip,RandomVerticalFlip\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5dd3f-6a08-4cfc-afcb-6b5ab2e7faf5",
   "metadata": {},
   "source": [
    "### Select device to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4d21f1-1f24-4d1d-ab3d-f7fc1e6e6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957a094-feea-416b-ab12-1cd5d2e5836c",
   "metadata": {},
   "source": [
    "## GTSRB Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8d7f9d-f8dd-4088-98b5-74f67ae13da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRB(Dataset):\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 split: str,\n",
    "                 transform: Optional[Callable] = None):\n",
    "       \n",
    "        \n",
    "        \n",
    "        self.base_folder = pathlib.Path(root)\n",
    "        self.csv_file = self.base_folder / ('Train.csv' if split =='train' else 'Test.csv')\n",
    "        \n",
    "        \n",
    "        with open(str(self.csv_file)) as csvfile:\n",
    "           samples = [(str(self.base_folder / row['Path']),int(row['ClassId'])) \n",
    "            for row in csv.DictReader(csvfile,delimiter=',',skipinitialspace=True)\n",
    "                ]\n",
    "\n",
    "\n",
    "        self.samples = samples\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return 20\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple:\n",
    "        path,classId =  self.samples[index]\n",
    "        sample = PIL.Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample,classId\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1aec28-b612-415f-88cb-a49f3e2958ae",
   "metadata": {},
   "source": [
    "## Load pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c64370-6f18-4203-94d2-576f54c22b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRB_MODEL(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(GTSRB_MODEL,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "      \n",
    "        self.metrics = {}\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "       \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3)\n",
    "        self.conv6 = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "       \n",
    "       \n",
    "\n",
    "        self.l1 = nn.Linear(1024*4*4,512)\n",
    "        self.l2 = nn.Linear(512,128)\n",
    "        self.batchnorm4 = nn.LayerNorm(128)\n",
    "        self.l3 = nn.Linear(128,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        conv = self.conv1(input)\n",
    "        conv = self.conv2(conv)\n",
    "        batchnorm = self.relu(self.batchnorm1(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv3(maxpool)\n",
    "        conv = self.conv4(conv)\n",
    "        batchnorm = self.relu(self.batchnorm2(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv5(maxpool)\n",
    "        conv = self.conv6(conv)\n",
    "        batchnorm = self.relu(self.batchnorm3(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        flatten = self.flatten(maxpool)\n",
    "        \n",
    "        dense_l1 = self.l1(flatten)\n",
    "        dropout = self.dropout3(dense_l1)\n",
    "        dense_l2 = self.l2(dropout)\n",
    "        batchnorm = self.batchnorm4(dense_l2)\n",
    "        dropout = self.dropout2(batchnorm)\n",
    "        output = self.l3(dropout)\n",
    "        \n",
    "       \n",
    "        return output\n",
    "    \n",
    "    def training_metrics(self,positives,data_size,loss):\n",
    "        acc = positives/data_size\n",
    "        return loss,acc\n",
    "    \n",
    "    def validation_metrics(self,validation_data,loss_function):\n",
    "       data_size = len(validation_data)\n",
    "       correct_predictions = 0\n",
    "       total_samples = 0\n",
    "       val_loss = 0\n",
    "\n",
    "       model = self.eval()\n",
    "       with torch.no_grad() : \n",
    "        for step,(input,label) in enumerate(validation_data):\n",
    "            input,label = input.to(device),label.to(device)\n",
    "            prediction = model.forward(input)\n",
    "            loss = loss_function(prediction,label)\n",
    "            val_loss = loss.item()\n",
    "            _,predicted = torch.max(prediction,1)\n",
    "            correct_predictions += (predicted == label).sum().item()\n",
    "            total_samples += label.size(0)\n",
    "\n",
    "       val_acc = correct_predictions/total_samples\n",
    "\n",
    "       return val_loss,val_acc\n",
    "\n",
    "    def history(self):\n",
    "        return self.metrics\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def compile(self,train_data,validation_data,epochs,loss_function,optimizer,learning_rate_scheduler):\n",
    "        val_acc_list = []\n",
    "        val_loss_list = []\n",
    "\n",
    "        train_acc_list = []\n",
    "        train_loss_list = []\n",
    "\n",
    "        learning_rate_list = []\n",
    "\n",
    "        print('training started ...')\n",
    "        STEPS = len(train_data)\n",
    "        for epoch in range(epochs):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            learning_rate_list.append(lr)\n",
    "            correct_predictions = 0\n",
    "            total_examples = 0\n",
    "            loss = 0\n",
    "            with tqdm.trange(STEPS) as progress:\n",
    "\n",
    "                for step,(input,label) in enumerate(train_loader):\n",
    "\n",
    "                    input,label = input.to(device),label.to(device)\n",
    "                    prediction = self.forward(input)\n",
    "\n",
    "                    _, predicted = torch.max(prediction, 1)\n",
    "                    correct_predictions += (predicted == label).sum().item()\n",
    "                    total_examples += label.size(0)\n",
    "                    l = loss_function(prediction,label)\n",
    "                    loss = l.item()\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    progress.colour = 'green'\n",
    "                    progress.desc = f'Epoch [{epoch}/{EPOCHS}], Step [{step}/{STEPS}], Learning Rate [{lr}], Loss [{\"{:.4f}\".format(l)}], Accuracy [{\"{:.4f}\".format(correct_predictions/total_examples)}]'\n",
    "                    progress.update(1)\n",
    "\n",
    "            training_loss,training_acc = self.training_metrics(correct_predictions,total_examples,loss)\n",
    "            train_acc_list.append(training_acc)\n",
    "            train_loss_list.append(training_loss)\n",
    "\n",
    "            val_loss, val_acc = self.validation_metrics(validation_data,loss_function)\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            print(f'val_accuracy [{val_acc}], val_loss [{val_loss}]')\n",
    "\n",
    "            \n",
    "            learning_rate_scheduler.step()\n",
    "        \n",
    "        metrics_dict = {\n",
    "                'train_acc':train_acc_list,\n",
    "                'train_loss':train_loss_list,\n",
    "                'val_acc':val_acc_list,\n",
    "                'val_loss':val_loss_list,\n",
    "                'learning_rate':optimizer.param_groups[0][\"lr\"]\n",
    "            }\n",
    "        self.metrics = metrics_dict\n",
    "        print('training complete !')    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32cb3c34-4198-441d-ad45-fcc0eafb7600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTSRB_MODEL(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (dropout3): Dropout(p=0.3, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batchnorm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Linear(in_features=16384, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (batchnorm4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (l3): Linear(in_features=128, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('gstrb-99-saved_model.pkl','rb')\n",
    "model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e63092-b589-4660-824e-f4b8945f6045",
   "metadata": {},
   "source": [
    "## FGSM Attack Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6cbafe-f99d-4000-ab01-bd0345654629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack_l2(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    normalized_data_grad = data_grad / data_grad.norm()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*normalized_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98be77-f3d7-4204-810b-626940acace9",
   "metadata": {},
   "source": [
    "## L2 FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e00fb2f-f298-4eca-9d24-382c3456c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|\u001b[31m███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████\u001b[0m| 39209/39209 [09:12<00:00, 70.97it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generate_adversarial_dataset_fgsm_l2(model, device, test_dataloader, epsilon):\n",
    "    adv_examples = []\n",
    "\n",
    "    with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "        # Loop over all examples in test set\n",
    "        for data, target in test_dataloader:\n",
    "            # Send the data and label to the device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Set requires_grad attribute of tensor. Important for Attack\n",
    "            data.requires_grad = True\n",
    "            # Forward pass the data through the model\n",
    "            output = model(data)\n",
    "            init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "            # if init_pred.item() != target.item():\n",
    "            #     adv_examples.append((data.squeeze().detach().cpu(), target))\n",
    "            #     progress.update(1)\n",
    "            #     continue\n",
    "            # Calculate the loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            # Calculate gradients of model in backward pass\n",
    "            loss.backward()\n",
    "            # Collect ``datagrad``\n",
    "            data_grad = data.grad.data\n",
    "            # Call FGSM Attack\n",
    "            perturbed_data = fgsm_attack_l2(data, epsilon, data_grad)\n",
    "            # Re-classify the perturbed image\n",
    "            output = model(perturbed_data)\n",
    "            adv_ex = perturbed_data.squeeze().detach().cpu()\n",
    "            adv_examples.append((adv_ex, target.detach().cpu().item()))\n",
    "            progress.desc = f'Progress: '\n",
    "            progress.update(1)\n",
    "\n",
    "    save_loc = f\"./pkl2/L2_epsilon2_train_advdata.pkl\"\n",
    "    with open(save_loc,'wb') as output_file:\n",
    "        pickle.dump(adv_examples,output_file)\n",
    "    return adv_examples\n",
    "\n",
    "transforms = Compose([\n",
    "    Resize([50,50]),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "traindataset = GTSRB(root='dataset',split=\"train\", transform=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(traindataset)\n",
    "\n",
    "adv_data = generate_adversarial_dataset_fgsm_l2(model, device, train_dataloader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8dcf0a-06f7-439c-978a-d0d6764ff2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.0008\n",
    "INPUT_DIM = 3*50*50\n",
    "OUTPUT_DIM = 43\n",
    "model = GTSRB_MODEL(INPUT_DIM,OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = Adam(params=model.parameters(),lr=LEARNING_RATE)\n",
    "lr_s = lr_scheduler.LinearLR(optimizer,start_factor=1.0,end_factor=0.5,total_iters=10)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e454bc-a0df-4f29-a1a1-29224d6f027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_file = open('./pkl2/L2_epsilon2_train_advdata.pkl','rb')\n",
    "adv_data = pickle.load(adv_data_file)\n",
    "adv_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354d16d4-a825-4817-9f08-a5f13d55a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDataset:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx][0],self.data[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b1e5e7-c593-4f28-a10e-c1e1cdda07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    Resize([50,50]),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "traindataset = GTSRB(root='dataset',split=\"train\", transform=transforms)\n",
    "\n",
    "base_folder = pathlib.Path('dataset')\n",
    "csv_file = base_folder / ('Train.csv')\n",
    "\n",
    "\n",
    "with open(str(self.csv_file)) as csvfile:\n",
    "   samples = [(str(self.base_folder / row['Path']),int(row['ClassId'])) \n",
    "    for row in csv.DictReader(csvfile,delimiter=',',skipinitialspace=True)\n",
    "        ]\n",
    "\n",
    "train_adv_dataset = AdversarialDataset(adv_data[:31000])\n",
    "val_adv_dataset = AdversarialDataset(adv_data[31000:])\n",
    "train_loader = DataLoader(train_adv_dataset, batch_size = 64,shuffle=True)\n",
    "val_loader = DataLoader(val_adv_dataset, batch_size = 64,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa277208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTSRB_MODEL(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (dropout3): Dropout(p=0.3, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batchnorm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Linear(in_features=16384, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (batchnorm4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (l3): Linear(in_features=128, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('gstrb-99-saved_model.pkl','rb')\n",
    "model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e9dbe26-2008-45d0-93dc-e4a64bf55ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/20], Step [484/485], Learning Rate [0.0008], Loss [3.6599], Accuracy [0.5305]: 100%|\u001b[32m█████████████████████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 55.60it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [2.0912258625030518]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [484/485], Learning Rate [0.00076], Loss [2.2750], Accuracy [0.5305]: 100%|\u001b[32m████████████████████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 56.45it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.956358551979065]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Step [484/485], Learning Rate [0.00072], Loss [3.4997], Accuracy [0.5305]: 100%|\u001b[32m████████████████████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 56.39it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [0.8660922050476074]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Step [484/485], Learning Rate [0.00068], Loss [3.1209], Accuracy [0.5305]: 100%|\u001b[32m████████████████████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 56.25it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [0.9137484431266785]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Step [484/485], Learning Rate [0.00064], Loss [2.2788], Accuracy [0.5305]: 100%|\u001b[32m████████████████████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 55.23it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [2.7526793479919434]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Step [484/485], Learning Rate [0.0006000000000000001], Loss [5.1470], Accuracy [0.5305]: 100%|\u001b[32m██████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 56.30it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.3679890632629395]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Step [484/485], Learning Rate [0.0005600000000000001], Loss [2.9563], Accuracy [0.5305]: 100%|\u001b[32m██████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 56.00it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.8349237442016602]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Step [484/485], Learning Rate [0.0005200000000000001], Loss [3.1730], Accuracy [0.5305]: 100%|\u001b[32m██████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 55.82it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [2.087939500808716]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Step [484/485], Learning Rate [0.00048000000000000007], Loss [2.9377], Accuracy [0.5305]: 100%|\u001b[32m█████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 55.30it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.3984570503234863]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Step [484/485], Learning Rate [0.00044000000000000007], Loss [2.8551], Accuracy [0.5305]: 100%|\u001b[32m█████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 54.10it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.2290706634521484]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [3.1084], Accuracy [0.5305]: 100%|\u001b[32m█████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 57.63it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [0.9397948384284973]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [3.1691], Accuracy [0.5305]: 100%|\u001b[32m█████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 54.37it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.3178845643997192]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [2.8046], Accuracy [0.5305]: 100%|\u001b[32m█████████████████████████████\u001b[0m| 485/485 [00:08<00:00, 54.28it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [2.540303945541382]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [3.9261], Accuracy [0.5305]: 100%|\u001b[32m█████████████████████████████\u001b[0m| 485/485 [00:09<00:00, 53.28it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.970440149307251]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Step [484/485], Learning Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.887372374534607]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [3.2729], Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.3792662620544434]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [3.1480], Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [3.0044679641723633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [3.9726], Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.3964910507202148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [5.3848], Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [1.805966854095459]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Step [484/485], Learning Rate [0.0004000000000000001], Loss [2.9539], Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy [0.6614691192593495], val_loss [2.301590919494629]\n",
      "training complete !\n"
     ]
    }
   ],
   "source": [
    "model.compile(train_data=train_loader,validation_data=val_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer,learning_rate_scheduler=lr_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5400c196-bb56-42e8-9ab2-4516da8e3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('robust_model.pkl','wb') as outfile:\n",
    "    pickle.dump(model.cpu(),outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7265a93-d71a-4d39-ada7-0834efa26e12",
   "metadata": {},
   "source": [
    "## Test accuracy on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a91507ab-3472-4840-adaf-fd8eb26a7daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing size : 12630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.9898653998416469 : 100%|\u001b[31m███████\u001b[0m| 12630/12630 [00:47<00:00, 267.07it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose([\n",
    "    Resize([50,50]),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "\n",
    "testdata = GTSRB(root='./dataset',split='test',transform=transforms)\n",
    "print('testing size :',len(testdata))\n",
    "test_dataloader = DataLoader(testdata)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model = model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input,label = input.to(device),label.to(device)\n",
    "        y_true.append(label.item())\n",
    "        prediction = model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred.append(prediction.item())\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d0e18-00b6-4c1f-9f15-70735ab0830e",
   "metadata": {},
   "source": [
    "### Dataset class for loading adversarial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6437e66-2700-4668-bd6d-f896b68d950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRB_adv(Dataset):\n",
    "    def __init__(self,file):\n",
    "        f = open(file,'rb')\n",
    "        self.data = pickle.load(f)\n",
    "        f.close()\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx][0],self.data[idx][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7b82c-b33a-4efc-b799-36aab0a657dc",
   "metadata": {},
   "source": [
    "## Test accuracy on L2 adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cb0d00b-5564-495b-ba18-28a0b7e131e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing size : 12630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.4927949326999208 : 100%|\u001b[31m███████\u001b[0m| 12630/12630 [00:43<00:00, 287.53it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "testdata = GTSRB_adv('./pkl/2_generate_adversarial_dataset_fgsm_l2_{}.pkl')\n",
    "print('testing size :',len(testdata))\n",
    "test_dataloader = DataLoader(testdata)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model = model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input,label = input.to(device),label.to(device)\n",
    "        y_true.append(label.item())\n",
    "        prediction = model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred.append(prediction.item())\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bd6cd-c321-4697-ab4f-1d81b4094425",
   "metadata": {},
   "source": [
    "## Test accuracy on small multistep adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f539898-a1cc-41fe-9d19-bb38a1230a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing size : 12630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.16904196357878068 : 100%|\u001b[31m██████\u001b[0m| 12630/12630 [00:42<00:00, 294.41it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "testdata = GTSRB_adv(\"./pkl/2_generate_adversarial_dataset_fgsm_multistep_{'num_epochs': 20}.pkl\")\n",
    "print('testing size :',len(testdata))\n",
    "test_dataloader = DataLoader(testdata)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model = model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input,label = input.to(device),label.to(device)\n",
    "        y_true.append(label.item())\n",
    "        prediction = model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred.append(prediction.item())\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
